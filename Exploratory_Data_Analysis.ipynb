{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faulthandler \n",
    "from pyspark.sql import SparkSession \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType, DateType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import col, array_contains, isnan, when, count\n",
    "from pyspark.sql.functions import lit, concat_ws, concat, collect_list, udf\n",
    "from pyspark.sql.functions import countDistinct\n",
    "import plotly.express as px\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark 접속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faulthandler.enable()   \n",
    "spark = SparkSession.builder.master('local').appName('Python Spark SQL Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/workspace/data/2019-Dec.csv\n",
      "D:/workspace/data/2019-Nov.csv\n",
      "D:/workspace/data/2019-Oct.csv\n",
      "D:/workspace/data/2020-Apr.csv\n",
      "D:/workspace/data/2020-Feb.csv\n",
      "D:/workspace/data/2020-Jan.csv\n",
      "D:/workspace/data/2020-Mar.csv\n"
     ]
    }
   ],
   "source": [
    "def search(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    file_list = []\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        print(full_filename)\n",
    "        file_list.append(full_filename)\n",
    "    return file_list\n",
    "data_path_list = search(\"D:/workspace/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스키마 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"event_time\",TimestampType(),True) \\\n",
    "      .add(\"event_type\",StringType(),True) \\\n",
    "      .add(\"product_id\",StringType(),True) \\\n",
    "      .add(\"category_id\",StringType(),True) \\\n",
    "      .add(\"category_code\",StringType(),True) \\\n",
    "      .add(\"brand\",StringType(),True) \\\n",
    "      .add(\"price\",DoubleType(),True) \\\n",
    "      .add(\"user_id\",StringType(),True) \\\n",
    "      .add(\"user_session\",StringType(),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA MERGE\n",
    "    - 2019년 10월 ~ 2020년 4월"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(data_path_list):\n",
    "    file_path = x\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .option('delimiter', ',') \\\n",
    "      .schema(schema) \\\n",
    "      .load(file_path)\n",
    "    if i == 0:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        merged_df = merged_df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|  brand|  price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "|2019-12-01 09:00:00|      view|   1005105|2232732093077520756|construction.tool...|  apple|1302.48|556695836|ca5eefc5-11f9-450...|\n",
      "|2019-12-01 09:00:00|      view|  22700068|2232732091643068746|                NULL|  force| 102.96|577702456|de33debe-c7bf-44e...|\n",
      "|2019-12-01 09:00:01|      view|   2402273|2232732100769874463|appliances.person...|  bosch| 313.52|539453785|5ee185a7-0689-4a3...|\n",
      "|2019-12-01 09:00:02|  purchase|  26400248|2053013553056579841|computers.periphe...|   NULL| 132.31|535135317|61792a26-672f-4e6...|\n",
      "|2019-12-01 09:00:02|      view|  20100164|2232732110089618156|    apparel.trousers|   nika| 101.68|517987650|906c6ca8-ff5c-419...|\n",
      "|2019-12-01 09:00:02|      view| 100008256|2053013561185141473|accessories.umbrella|   ikea| 163.56|542860793|a1bcb550-1065-476...|\n",
      "|2019-12-01 09:00:02|      view|  21400264|2053013561579406073|  electronics.clocks|   NULL|  88.81|538021416|e88f77cc-e75e-4e9...|\n",
      "|2019-12-01 09:00:03|      view|   1005239|2232732093077520756|construction.tool...| xiaomi| 256.38|525740700|370e8c88-3d07-41d...|\n",
      "|2019-12-01 09:00:04|      view|   5100885|2053013553375346967|  computers.notebook|    jet|  20.57|512509221|4227259f-1c4c-41d...|\n",
      "|2019-12-01 09:00:04|      view|  26205399|2232732081585127530|construction.comp...|   NULL| 179.16|553345124|58c692ff-c7a9-4e3...|\n",
      "|2019-12-01 09:00:04|      view|  22900009|2053013553375346967|  computers.notebook|  vegas|  49.94|554369617|a4481ea8-9a20-442...|\n",
      "|2019-12-01 09:00:04|      view|   1004233|2232732093077520756|construction.tool...|  apple|1312.52|579969851|90aca71c-ed8a-467...|\n",
      "|2019-12-01 09:00:05|      view|  22700202|2232732091643068746|                NULL|  stels| 171.18|575086722|05a5e4f4-1865-4b0...|\n",
      "|2019-12-01 09:00:06|      view|   1004856|2232732093077520756|construction.tool...|samsung| 124.11|532554953|4bd14129-caad-4de...|\n",
      "|2019-12-01 09:00:06|      view|   3701309|2053013565983425517|appliances.enviro...|polaris|  89.32|543733099|a65116f4-ac53-4a4...|\n",
      "|2019-12-01 09:00:07|      view|  11500445|2053013552259662037|computers.compone...| xiaomi|  27.77|526844203|5e62045f-58f7-442...|\n",
      "|2019-12-01 09:00:07|      view|   1005105|2232732093077520756|construction.tool...|  apple|1302.48|562071412|822749fe-49f1-4a9...|\n",
      "|2019-12-01 09:00:07|      view|   1003489|2232732093077520756|construction.tool...| huawei| 205.67|543826485|b251cbde-4373-497...|\n",
      "|2019-12-01 09:00:07|      view|   1480790|2053013553341792533|  electronics.clocks| lenovo| 342.82|577653879|a971d966-d601-4d8...|\n",
      "|2019-12-01 09:00:07|      view|   4804718|2232732079706079299|       sport.bicycle|  apple| 329.14|579969767|e159d1a1-6668-477...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark table 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.createOrReplaceTempView(\"ecommerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 카테코리 id가 NULL 값인 것들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda1 = spark.sql(\"\"\"SELECT COUNT(*) AS CNT\n",
    "FROM ecommerce\n",
    "WHERE category_id is NULL\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|CNT|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eda1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. remove_from_cart 항목이 있는지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda2 = spark.sql(\"\"\"SELECT COUNT(*) AS CNT\n",
    "FROM ecommerce\n",
    "WHERE event_type = 'remove_from_cart'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 카테고리 아이디 별 평균 물품 금액"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda3 = spark.sql(\"\"\"SELECT category_id, category_code, AVG(price) AS AVG_PRICE_PER_CATEGORY\n",
    "FROM ecommerce\n",
    "GROUP BY category_code, category_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 카테고리 아이디 별 평균 구매 물품 금액"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda4 = spark.sql(\"\"\"SELECT category_id, category_code, AVG(price) AS AVG_PRICE_PER_CATEGORY\n",
    "FROM ecommerce\n",
    "WHERE event_type = 'purchase'\n",
    "GROUP BY category_id\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 월별 USER 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda5 = spark.sql(\"\"\"\n",
    "                               SELECT MONTH(event_time) AS MONTH, COUNT(DISTINCT user_id) AS CNT\n",
    "                              FROM ecommerce\n",
    "                              GROUP BY MONTH\n",
    "                          \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. EVENT_TYPE 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda6 = spark.sql(\"SELECT event_type, count(*) AS count FROM ecommerce GROUP BY event_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Spark DataFrame을 Pandas DataFrame으로 변환\n",
    "pandas_df = eda6.toPandas()\n",
    "\n",
    "# 이벤트 타입을 view -> cart -> purchase 순으로 정렬\n",
    "pandas_df = pandas_df.set_index('event_type').reindex(['view', 'cart', 'purchase']).reset_index()\n",
    "\n",
    "# 원 그래프 생성\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)  # 첫 번째 그래프는 원 그래프\n",
    "plt.pie(pandas_df['count'], labels=pandas_df['event_type'], autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Event Type Distribution (Pie Chart)')\n",
    "\n",
    "# 막대 그래프 생성\n",
    "plt.subplot(1, 2, 2)  # 두 번째 그래프는 막대 그래프\n",
    "plt.bar(pandas_df['event_type'], pandas_df['count'], color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Event Type Counts (Bar Chart)')\n",
    "plt.xlabel('Event Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 그래프 보여주기\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. user_session 별 view, purchase, cart의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda7 = spark.sql('''WITH source AS (\n",
    "                                                SELECT user_session, event_type, COUNT(event_time) AS event_count\n",
    "                                                FROM ecommerce\n",
    "                                                GROUP BY user_session, event_type\n",
    "                                                )\n",
    "                            SELECT \n",
    "                                user_session,\n",
    "                                SUM(CASE WHEN event_type = 'view' THEN event_count ELSE 0 END) AS view,\n",
    "                                SUM(CASE WHEN event_type = 'cart' THEN event_count ELSE 0 END) AS cart,\n",
    "                                SUM(CASE WHEN event_type = 'purchase' THEN event_count ELSE 0 END) AS purchase\n",
    "                            FROM source\n",
    "                            GROUP BY user_session\n",
    "                            ORDER BY user_session\n",
    "                        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. user_session의 NULL 여부 및 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda8 = spark.sql(\"\"\"\n",
    "SELECT COUNT(*) AS CNT\n",
    "FROM ecommerce\n",
    "WHERE user_session is NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 카테고리 갯수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카테고리 대분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda9_1 = spark.sql(\"\"\"SELECT DISTINCT (SUBSTRING_INDEX(category_code, '.', 1)) \n",
    "                   FROM ecommerce\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda9_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대분류 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda9_2 = spark.sql(\"\"\"SELECT COUNT(DISTINCT (SUBSTRING_INDEX(category_code, '.', 1))) AS CNT \n",
    "                   FROM ecommerce\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda9_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카테고리 중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda9_3= spark.sql(\"\"\"SELECT DISTINCT (SUBSTRING_INDEX(category_code, '.', 2)) AS cnt \n",
    "          FROM ecommerce\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda9_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카테고리 중분류 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda9_4 = spark.sql(\"\"\"SELECT COUNT(DISTINCT (SUBSTRING_INDEX(category_code, '.', 2))) AS cnt \n",
    "          FROM ecommerce\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda9_4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 요일별 판매량, 판매액, paying User\n",
    "DAYOFWEEK => 일요일 1, 월요일 2, 화요일 3, 수요일 4, 목요일 5, 금요일 6, 토요일 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda10 = spark.sql(\"\"\"\n",
    "                                SELECT CASE DAYOFWEEK(event_time)\n",
    "                                            WHEN '1' THEN 'SUN'\n",
    "                                            WHEN '2' THEN 'MON'\n",
    "                                            WHEN '3' THEN 'TUE'\n",
    "                                            WHEN '4' THEN 'WED'\n",
    "                                            WHEN '5' THEN 'THU' \n",
    "                                            WHEN '6' THEN 'FRI' \n",
    "                                            WHEN '7' THEN 'SAT' \n",
    "                                        END\n",
    "                                                AS dayOfweek,\n",
    "                                        COUNT(*) AS total_sales_amount,\n",
    "                                        FLOOR(SUM(price)) AS total_sales_price,\n",
    "                                        COUNT(DISTINCT user_id) AS paying_user_cnt\n",
    "                                FROM ecommerce\n",
    "                                WHERE event_type = 'purchase'\n",
    "                                GROUP BY dayOfweek\n",
    "                                ORDER BY total_sales_amount DESC\n",
    "                             \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 월별 판매량, 판매액, paying User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda11 = spark.sql(\"\"\"\n",
    "                                SELECT MONTH(event_time) AS MONTH,\n",
    "                                        COUNT(*) AS total_sales_amount,\n",
    "                                        FLOOR(SUM(price)) AS total_sales_price,\n",
    "                                        COUNT(DISTINCT user_id) AS paying_user_cnt\n",
    "                                FROM ecommerce\n",
    "                                WHERE event_type = 'purchase'\n",
    "                                GROUP BY MONTH\n",
    "                                ORDER BY total_sales_amount DESC\n",
    "                             \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda11.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
