{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faulthandler \n",
    "from pyspark.sql import SparkSession \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType, DateType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import col, array_contains, isnan, when, count\n",
    "from pyspark.sql.functions import lit, concat_ws, concat, collect_list, udf\n",
    "from pyspark.sql.functions import countDistinct\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark 접속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faulthandler.enable()   \n",
    "spark = SparkSession.builder.master('local').appName(\"Python Spark SQL Practice\").config(\"spark.driver.maxResultSize\", \"64g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/workspace/data/2019-Dec.csv\n",
      "D:/workspace/data/2019-Nov.csv\n",
      "D:/workspace/data/2019-Oct.csv\n",
      "D:/workspace/data/2020-Apr.csv\n",
      "D:/workspace/data/2020-Feb.csv\n",
      "D:/workspace/data/2020-Jan.csv\n",
      "D:/workspace/data/2020-Mar.csv\n"
     ]
    }
   ],
   "source": [
    "def search(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    file_list = []\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        print(full_filename)\n",
    "        file_list.append(full_filename)\n",
    "    return file_list\n",
    "data_path_list = search(\"D:/workspace/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스키마 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"event_time\",TimestampType(),True) \\\n",
    "      .add(\"event_type\",StringType(),True) \\\n",
    "      .add(\"product_id\",StringType(),True) \\\n",
    "      .add(\"category_id\",StringType(),True) \\\n",
    "      .add(\"category_code\",StringType(),True) \\\n",
    "      .add(\"brand\",StringType(),True) \\\n",
    "      .add(\"price\",DoubleType(),True) \\\n",
    "      .add(\"user_id\",StringType(),True) \\\n",
    "      .add(\"user_session\",StringType(),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data merge\n",
    "    - 2019년 10월 ~ 2020년 4월"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(data_path_list):\n",
    "    file_path = x\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .option('delimiter', ',') \\\n",
    "      .schema(schema) \\\n",
    "      .load(file_path)\n",
    "    if i == 0:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        merged_df = merged_df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|  brand|  price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "|2019-12-01 09:00:00|      view|   1005105|2232732093077520756|construction.tool...|  apple|1302.48|556695836|ca5eefc5-11f9-450...|\n",
      "|2019-12-01 09:00:00|      view|  22700068|2232732091643068746|                NULL|  force| 102.96|577702456|de33debe-c7bf-44e...|\n",
      "|2019-12-01 09:00:01|      view|   2402273|2232732100769874463|appliances.person...|  bosch| 313.52|539453785|5ee185a7-0689-4a3...|\n",
      "|2019-12-01 09:00:02|  purchase|  26400248|2053013553056579841|computers.periphe...|   NULL| 132.31|535135317|61792a26-672f-4e6...|\n",
      "|2019-12-01 09:00:02|      view|  20100164|2232732110089618156|    apparel.trousers|   nika| 101.68|517987650|906c6ca8-ff5c-419...|\n",
      "|2019-12-01 09:00:02|      view| 100008256|2053013561185141473|accessories.umbrella|   ikea| 163.56|542860793|a1bcb550-1065-476...|\n",
      "|2019-12-01 09:00:02|      view|  21400264|2053013561579406073|  electronics.clocks|   NULL|  88.81|538021416|e88f77cc-e75e-4e9...|\n",
      "|2019-12-01 09:00:03|      view|   1005239|2232732093077520756|construction.tool...| xiaomi| 256.38|525740700|370e8c88-3d07-41d...|\n",
      "|2019-12-01 09:00:04|      view|   5100885|2053013553375346967|  computers.notebook|    jet|  20.57|512509221|4227259f-1c4c-41d...|\n",
      "|2019-12-01 09:00:04|      view|  26205399|2232732081585127530|construction.comp...|   NULL| 179.16|553345124|58c692ff-c7a9-4e3...|\n",
      "|2019-12-01 09:00:04|      view|  22900009|2053013553375346967|  computers.notebook|  vegas|  49.94|554369617|a4481ea8-9a20-442...|\n",
      "|2019-12-01 09:00:04|      view|   1004233|2232732093077520756|construction.tool...|  apple|1312.52|579969851|90aca71c-ed8a-467...|\n",
      "|2019-12-01 09:00:05|      view|  22700202|2232732091643068746|                NULL|  stels| 171.18|575086722|05a5e4f4-1865-4b0...|\n",
      "|2019-12-01 09:00:06|      view|   1004856|2232732093077520756|construction.tool...|samsung| 124.11|532554953|4bd14129-caad-4de...|\n",
      "|2019-12-01 09:00:06|      view|   3701309|2053013565983425517|appliances.enviro...|polaris|  89.32|543733099|a65116f4-ac53-4a4...|\n",
      "|2019-12-01 09:00:07|      view|  11500445|2053013552259662037|computers.compone...| xiaomi|  27.77|526844203|5e62045f-58f7-442...|\n",
      "|2019-12-01 09:00:07|      view|   1005105|2232732093077520756|construction.tool...|  apple|1302.48|562071412|822749fe-49f1-4a9...|\n",
      "|2019-12-01 09:00:07|      view|   1003489|2232732093077520756|construction.tool...| huawei| 205.67|543826485|b251cbde-4373-497...|\n",
      "|2019-12-01 09:00:07|      view|   1480790|2053013553341792533|  electronics.clocks| lenovo| 342.82|577653879|a971d966-d601-4d8...|\n",
      "|2019-12-01 09:00:07|      view|   4804718|2232732079706079299|       sport.bicycle|  apple| 329.14|579969767|e159d1a1-6668-477...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark table 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.createOrReplaceTempView(\"ecommerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ACQUISITION(고객 유치) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DAU (Daily Active User)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau = spark.sql(\"\"\"\n",
    "SELECT DATE(event_time) AS event_date, COUNT(DISTINCT user_id) AS DAU\n",
    "FROM ecommerce\n",
    "GROUP BY event_date\n",
    "ORDER BY event_date\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau_df = dau.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau_df.to_csv(\"dau.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAU 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) MAU(Monthly Active User)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAU 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mau = spark.sql(\"\"\"\n",
    "SELECT DATE_FORMAT(event_time, 'yyyy-MM') AS event_month, COUNT(DISTINCT user_id) AS MAU\n",
    "FROM ecommerce\n",
    "GROUP BY event_month\n",
    "ORDER BY event_month\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mau_df = mau.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mau_df.to_csv('mau.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAU 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ACTIVATION(활성화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DT(Duration Time, 체류 시간)\n",
    "    - DT가 1일 이하"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_under_1day = spark.sql(\"\"\"WITH paying_sessions AS (\n",
    "    SELECT DISTINCT user_session\n",
    "    FROM ecommerce\n",
    "    WHERE event_type = 'purchase'\n",
    "), \n",
    "\n",
    "session_time AS (\n",
    "    SELECT e.user_session, \n",
    "           MAX(e.event_time) AS max_session_time,\n",
    "           MIN(e.event_time) AS min_session_time\n",
    "    FROM ecommerce e\n",
    "    JOIN paying_sessions p ON e.user_session = p.user_session\n",
    "    GROUP BY e.user_session\n",
    ")\n",
    "\n",
    "SELECT s.user_session, \n",
    "       (UNIX_TIMESTAMP(s.max_session_time) - UNIX_TIMESTAMP(s.min_session_time)) AS duration_seconds\n",
    "FROM session_time s\n",
    "WHERE (UNIX_TIMESTAMP(s.max_session_time) - UNIX_TIMESTAMP(s.min_session_time)) < 86400\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_under_1day_df = duration_under_1day.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.267605e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.246561e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.153458e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.210000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.510000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.540000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.638500e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration_seconds\n",
       "count      5.267605e+06\n",
       "mean       6.246561e+02\n",
       "std        2.153458e+03\n",
       "min        0.000000e+00\n",
       "25%        1.210000e+02\n",
       "50%        2.510000e+02\n",
       "75%        5.540000e+02\n",
       "max        8.638500e+04"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_under_1day_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o154.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m total_session_num_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mSELECT COUNT(*) AS CNT\u001b[39;49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;43m                              FROM ecommerce\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;43m                              GROUP BY user_session\u001b[39;49m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;124;43m                              \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1244\u001b[0m \n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o154.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n"
     ]
    }
   ],
   "source": [
    "total_session_num_df = spark.sql(\"\"\"SELECT COUNT(*) AS CNT\n",
    "                              FROM ecommerce\n",
    "                              GROUP BY user_session\n",
    "                              \"\"\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"전체 session 중 1일 이하의 duration time을 갖고 있는 session의 비율은 : {len(duration_under_1day_df)/total_session_num_df.iloc[0,0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카테고리별 전환율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_category_df = spark.sql(\"\"\"SELECT DISTINCT (SUBSTRING_INDEX(category_code, '.', 1)) AS cnt \n",
    "          FROM ecommerce\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = main_category_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dataframes = []\n",
    "for x in tqdm(category_list):\n",
    "     funnel_per_category = spark.sql(f\"\"\"\n",
    "                    SELECT \n",
    "                         event_type, COUNT(*) AS CNT\n",
    "                    FROM \n",
    "                         ecommerce\n",
    "                    WHERE\n",
    "                         SUBSTRING_INDEX(category_code, '.', 1) = '{x[0]}'\n",
    "                         \n",
    "                    GROUP BY \n",
    "                         event_type\n",
    "                                   \n",
    "                    ORDER BY\n",
    "                         CNT DESC\n",
    "                         \n",
    "                    \n",
    "     \"\"\")\n",
    "     pdf = funnel_per_category.toPandas()\n",
    "     pdf[\"category_code\"] = x[0]\n",
    "     category_dataframes.append(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funnel_per_category = pd.concat(category_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funnel_per_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카테고리별 구매 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_list = [['medicine'],\n",
    " ['computers'],\n",
    " ['auto'],\n",
    " ['stationery'],\n",
    " ['sport'],\n",
    " ['apparel'],\n",
    " ['appliances'],\n",
    " ['country_yard'],\n",
    " ['furniture'],\n",
    " ['accessories'],\n",
    " ['kids'],\n",
    " ['electronics'],\n",
    " ['construction'],\n",
    " [None]]\n",
    "\n",
    "category_dataframes = []\n",
    "\n",
    "for x in tqdm(category_list):\n",
    "     purcahse_cycle_per_category = spark.sql(f\"\"\"\n",
    "                                                WITH third AS(\n",
    "                                                            WITH second AS(\n",
    "                                                                        WITH first AS(\n",
    "                                                                                    SELECT \n",
    "                                                                                          user_id,\n",
    "                                                                                          event_time\n",
    "                                                                                    FROM ecommerce\n",
    "                                                                                    WHERE SUBSTRING_INDEX(category_code, '.', 1) = '{x[0]}'\n",
    "                                                                                    AND event_type = 'purchase'\n",
    "                                                                                    )\n",
    "                                                                        SELECT \n",
    "                                                                              user_id,\n",
    "                                                                              event_time AS order_date,\n",
    "                                                                              LAG(event_time) OVER (\n",
    "                                                                                                      PARTITION BY user_id \n",
    "                                                                                                      ORDER BY event_time) AS prev_order_date      \n",
    "                                                                        FROM first\n",
    "                                                                        GROUP BY user_id, event_time\n",
    "                                                                        )\n",
    "                                                            SELECT\n",
    "                                                                        user_id,\n",
    "                                                                        AVG(DATEDIFF(order_date, prev_order_date)) AS user_avg_purchase_cycle\n",
    "                                                            FROM second\n",
    "                                                            WHERE prev_order_date IS NOT NULL\n",
    "                                                            GROUP BY user_id\n",
    "                                                      )\n",
    "                                                      SELECT \n",
    "                                                            AVG(user_avg_purchase_cycle) AS category_avg_purchase_cycle\n",
    "                                                      FROM third\n",
    "     \"\"\")\n",
    "     pdf = purcahse_cycle_per_category.toPandas()\n",
    "     pdf[\"category_code\"] = x[0]\n",
    "     category_dataframes.append(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency_purchase_per_category = pd.concat(category_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequency_purchase_per_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ltr(m):\n",
    "    query = f\"\"\"\n",
    "                    WITH month_users AS (\n",
    "                        SELECT user_id, MIN(event_time) AS first_join\n",
    "                        FROM ecommerce\n",
    "                        GROUP BY user_id\n",
    "                        HAVING MONTH(first_join) = {m}\n",
    "                    ),\n",
    "\n",
    "                    activity_summary AS (\n",
    "                        SELECT DATE_FORMAT(event_time, 'yyyy-MM') AS Month,\n",
    "                                COUNT(DISTINCT s.user_id) AS ActiveUser,\n",
    "                                COUNT(DISTINCT CASE WHEN s.event_type ='purchase' THEN s.user_id END) AS PayingUser,\n",
    "                                SUM(DISTINCT CASE WHEN s.event_type = 'purchase' THEN s.price END) AS PayingAmount\n",
    "                        FROM ecommerce s INNER JOIN month_users m\n",
    "                        ON s.user_id = m.user_id\n",
    "                        GROUP BY Month\n",
    "                    )\n",
    "\n",
    "                    SELECT Month, ActiveUser, PayingUser, PayingAmount,\n",
    "                            ROUND(PayingAmount / FIRST(ActiveUser) OVER (),2)  AS AmountPerFirstActiveUser\n",
    "                    FROM activity_summary \n",
    "                    ORDER BY Month\n",
    "                \"\"\"\n",
    "    return spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "months = ['10', '11', '12', '1', '2', '3', '4']\n",
    "ltrsstr = {}\n",
    "\n",
    "for month in months:\n",
    "    globals()[f\"ltrstr{month}_df\"] = calculate_ltr(month).toPandas()\n",
    "    ltrsstr[month] = globals()[f\"ltrstr{month}_df\"].AmountPerFirstActiveUser.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(ltrsstr.keys())\n",
    "y = list(ltrsstr.values())\n",
    "\n",
    "plt.plot(x, y, marker='o', color='skyblue', label=\"LTR\")\n",
    "plt.title(\"LTR and CAC\", fontsize=16)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(x)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrsstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = spark.sql(\"\"\"\n",
    "               WITH rr AS (\n",
    "                    SELECT user_id, COUNT(*) AS CNT\n",
    "                    FROM ecommerce\n",
    "                    WHERE event_type = 'purchase'\n",
    "                    GROUP BY user_id\n",
    "                    )\n",
    "               SELECT (SELECT COUNT(*) FROM rr WHERE CNT > 1) / COUNT(*)\n",
    "               FROM rr\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
