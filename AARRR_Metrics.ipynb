{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faulthandler \n",
    "from pyspark.sql import SparkSession \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType, DateType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import col, array_contains, isnan, when, count\n",
    "from pyspark.sql.functions import lit, concat_ws, concat, collect_list, udf\n",
    "from pyspark.sql.functions import countDistinct\n",
    "import plotly.express as px\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark 접속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faulthandler.enable()   \n",
    "spark = SparkSession.builder.master('local').appName('Python Spark SQL Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/workspace/data/2019-Dec.csv\n",
      "D:/workspace/data/2019-Nov.csv\n",
      "D:/workspace/data/2019-Oct.csv\n",
      "D:/workspace/data/2020-Apr.csv\n",
      "D:/workspace/data/2020-Feb.csv\n",
      "D:/workspace/data/2020-Jan.csv\n",
      "D:/workspace/data/2020-Mar.csv\n"
     ]
    }
   ],
   "source": [
    "def search(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    file_list = []\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        print(full_filename)\n",
    "        file_list.append(full_filename)\n",
    "    return file_list\n",
    "data_path_list = search(\"D:/workspace/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스키마 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "      .add(\"event_time\",TimestampType(),True) \\\n",
    "      .add(\"event_type\",StringType(),True) \\\n",
    "      .add(\"product_id\",StringType(),True) \\\n",
    "      .add(\"category_id\",StringType(),True) \\\n",
    "      .add(\"category_code\",StringType(),True) \\\n",
    "      .add(\"brand\",StringType(),True) \\\n",
    "      .add(\"price\",DoubleType(),True) \\\n",
    "      .add(\"user_id\",StringType(),True) \\\n",
    "      .add(\"user_session\",StringType(),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data merge\n",
    "    - 2019년 10월 ~ 2020년 4월"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(data_path_list):\n",
    "    file_path = x\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .option('delimiter', ',') \\\n",
    "      .schema(schema) \\\n",
    "      .load(file_path)\n",
    "    if i == 0:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        merged_df = merged_df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|  brand|  price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "|2019-12-01 09:00:00|      view|   1005105|2232732093077520756|construction.tool...|  apple|1302.48|556695836|ca5eefc5-11f9-450...|\n",
      "|2019-12-01 09:00:00|      view|  22700068|2232732091643068746|                NULL|  force| 102.96|577702456|de33debe-c7bf-44e...|\n",
      "|2019-12-01 09:00:01|      view|   2402273|2232732100769874463|appliances.person...|  bosch| 313.52|539453785|5ee185a7-0689-4a3...|\n",
      "|2019-12-01 09:00:02|  purchase|  26400248|2053013553056579841|computers.periphe...|   NULL| 132.31|535135317|61792a26-672f-4e6...|\n",
      "|2019-12-01 09:00:02|      view|  20100164|2232732110089618156|    apparel.trousers|   nika| 101.68|517987650|906c6ca8-ff5c-419...|\n",
      "|2019-12-01 09:00:02|      view| 100008256|2053013561185141473|accessories.umbrella|   ikea| 163.56|542860793|a1bcb550-1065-476...|\n",
      "|2019-12-01 09:00:02|      view|  21400264|2053013561579406073|  electronics.clocks|   NULL|  88.81|538021416|e88f77cc-e75e-4e9...|\n",
      "|2019-12-01 09:00:03|      view|   1005239|2232732093077520756|construction.tool...| xiaomi| 256.38|525740700|370e8c88-3d07-41d...|\n",
      "|2019-12-01 09:00:04|      view|   5100885|2053013553375346967|  computers.notebook|    jet|  20.57|512509221|4227259f-1c4c-41d...|\n",
      "|2019-12-01 09:00:04|      view|  26205399|2232732081585127530|construction.comp...|   NULL| 179.16|553345124|58c692ff-c7a9-4e3...|\n",
      "|2019-12-01 09:00:04|      view|  22900009|2053013553375346967|  computers.notebook|  vegas|  49.94|554369617|a4481ea8-9a20-442...|\n",
      "|2019-12-01 09:00:04|      view|   1004233|2232732093077520756|construction.tool...|  apple|1312.52|579969851|90aca71c-ed8a-467...|\n",
      "|2019-12-01 09:00:05|      view|  22700202|2232732091643068746|                NULL|  stels| 171.18|575086722|05a5e4f4-1865-4b0...|\n",
      "|2019-12-01 09:00:06|      view|   1004856|2232732093077520756|construction.tool...|samsung| 124.11|532554953|4bd14129-caad-4de...|\n",
      "|2019-12-01 09:00:06|      view|   3701309|2053013565983425517|appliances.enviro...|polaris|  89.32|543733099|a65116f4-ac53-4a4...|\n",
      "|2019-12-01 09:00:07|      view|  11500445|2053013552259662037|computers.compone...| xiaomi|  27.77|526844203|5e62045f-58f7-442...|\n",
      "|2019-12-01 09:00:07|      view|   1005105|2232732093077520756|construction.tool...|  apple|1302.48|562071412|822749fe-49f1-4a9...|\n",
      "|2019-12-01 09:00:07|      view|   1003489|2232732093077520756|construction.tool...| huawei| 205.67|543826485|b251cbde-4373-497...|\n",
      "|2019-12-01 09:00:07|      view|   1480790|2053013553341792533|  electronics.clocks| lenovo| 342.82|577653879|a971d966-d601-4d8...|\n",
      "|2019-12-01 09:00:07|      view|   4804718|2232732079706079299|       sport.bicycle|  apple| 329.14|579969767|e159d1a1-6668-477...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark table 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.createOrReplaceTempView(\"ecommerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ACQUISITION(고객 유치) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DAU (Daily Active User)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAU 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau = spark.sql(\"\"\"\n",
    "SELECT DATE(event_time) AS event_date, COUNT(DISTINCT user_id) AS DAU\n",
    "FROM ecommerce\n",
    "GROUP BY event_date\n",
    "ORDER BY event_date\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dau.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAU 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) MAU(Monthly Active User)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAU 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mau = spark.sql(\"\"\"\n",
    "SELECT DATE_FORMAT(event_time, 'yyyy-mm') AS event_month, COUNT(DISTINCT user_id) AS DAU\n",
    "FROM ecommerce\n",
    "GROUP BY event_month\n",
    "ORDER BY event_month\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mau.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAU 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ACTIVATION(활성화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) DT(Duration Time, 체류 시간)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = spark.sql('''SELECT user_session, \n",
    "                                MONTH(MAX(event_time)) AS DURATION_MONTH,\n",
    "                                DATE(MAX(event_time)) AS DURATION_DATE,\n",
    "                               MAX(event_time) - MIN(event_time) AS duration\n",
    "                  FROM ecommerce \n",
    "                  GROUP BY user_session\n",
    "                  '''\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 구매까지 걸리는 DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### duration 테이블 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration.createOrReplaceTempView('duration_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user_session 별로 view, purchase, cart의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_session = spark.sql('''WITH source AS (\n",
    "                                                SELECT user_session, event_type, COUNT(event_time) AS event_count\n",
    "                                                FROM ecommerce\n",
    "                                                GROUP BY user_session, event_type\n",
    "                                                )\n",
    "                            SELECT \n",
    "                                user_session,\n",
    "                                SUM(CASE WHEN event_type = 'view' THEN event_count ELSE 0 END) AS view,\n",
    "                                SUM(CASE WHEN event_type = 'cart' THEN event_count ELSE 0 END) AS cart,\n",
    "                                SUM(CASE WHEN event_type = 'purchase' THEN event_count ELSE 0 END) AS purchase\n",
    "                            FROM source\n",
    "                            GROUP BY user_session\n",
    "                            ORDER BY user_session\n",
    "                        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_session.createOrReplaceTempView('session_pivot_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구매 이력이 있는 섹션의 DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_session_avg_duration = spark.sql('''\n",
    "                                                SELECT MEAN(d.duration)\n",
    "                                                FROM duration_table AS d, \n",
    "                                                    (SELECT user_session, purchase\n",
    "                                                    FROM session_pivot_table\n",
    "                                                    WHERE purchase > 0)   AS p\n",
    "                                                WHERE d.user_session = p.user_session\n",
    "                                        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_session_avg_duration.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 월간 DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 월간 DT 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_month = spark.sql(\"\"\"\n",
    "WITH temp AS (\n",
    "    SELECT user_session, \n",
    "    MONTH(MAX(event_time)) AS DURATION_MONTH\n",
    "    DATE(MAX(event_time)) AS DURATION_DATE\n",
    "    MAX(event_time) - MIN(event_time) AS duration\n",
    "    FROM ecommerce \n",
    "    GROUP BY user_session\n",
    ")\n",
    "SELECT DURATION_DATE, AVG(duration) AS AVG_DURATION\n",
    "FROM temp\n",
    "GROUP BY DURATION_DATE\n",
    "ORDERY BY DURATION_DATE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) FUNNEL (EVENT_TYPE 별 COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNNEL 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel = spark.sql(\"\"\"\n",
    "                   SELECT \n",
    "                        event_type, COUNT(*) AS CNT\n",
    "                   FROM \n",
    "                        ecommerce\n",
    "                   GROUP BY \n",
    "                        event_type\n",
    "                   ORDER BY\n",
    "                        CNT DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funnel = funnel.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNNEL 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.funnel(data_frame= df_funnel, x = 'event_type', y = 'CNT')\n",
    "fig.update_traces(texttemplate= '%{value:,.0f}') # 숫자 형식 지정\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RETENTION(리텐션)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Monthly Retention\n",
    "    - 접속 했는지 여부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 월간 리텐션 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention_range_30 = spark.sql(\"\"\"\n",
    "                               WITH first as (\n",
    "                                    SELECT user_id, MIN(event_time) AS first_interaction_time\n",
    "                                    FROM ecommerce\n",
    "                                    GROUP BY user_id\n",
    "                                )\n",
    "\n",
    "                                SELECT MONTH(first_interaction_time) AS MONTH, FLOOR(MONTHS_BETWEEN(event_time, first_interaction_time)) AS diff_month,\n",
    "                                COUNT(DISTINCT s.user_id) as user_cnt\n",
    "                                FROM ecommerce s LEFT JOIN first f\n",
    "                                ON s.user_id = f.user_id\n",
    "                                GROUP BY MONTH, FLOOR(MONTHS_BETWEEN(event_time, first_interaction_time))\n",
    "                                ORDER BY diff_month\n",
    "\n",
    "                          \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention_range_30.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas, pivot 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retention_30 = retention_range_30.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retention_30_pivot = df_retention_30.pivot(index='MONTH',columns='diff_month',values='user_cnt').reindex([10,11,12,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retention_30_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### value 비율로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_retention_30_pivot)):\n",
    "    a = df_retention_30_pivot.iloc[i,0]\n",
    "    for j in range(len(df_retention_30_pivot.columns)):\n",
    "        if df_retention_30_pivot.isnull().iloc[i,j]:\n",
    "            pass\n",
    "        else:\n",
    "            df_retention_30_pivot.iloc[i,j] = round(df_retention_30_pivot.iloc[i,j] / a, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retention_30_pivot.to_csv(\"월간간리텐션.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 월간 리텐션 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(df_retention_30_pivot, annot=True, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[문제를 찾자]\n",
    "전반적으로 10, 11월 retention 좋음\n",
    "\n",
    "1~3월 폭망\n",
    "\n",
    "그 이유는 무엇일까???\n",
    "\n",
    "Q1.전체적인 건수가 줄어서???\n",
    "그럼 왜 줄어 들었을까????\n",
    "\n",
    "단순히 연초라서????\n",
    "Q2.연말에 폭풍 소비 후 연초에 살 필요성이 없음????\n",
    "\n",
    "아니면 제품군?????\n",
    "EX. 컴퓨터나 장기간 사용할 물건을 구매해서 다시 들어갈 일이 생기지 않았나???\n",
    "--> 블랙프레이데이를 이용해서 저런 것들을 구매 후 재방문으로는 이어지지 않았나???\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Weekly Retention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주간 리텐션 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention_range_7 = spark.sql(\"\"\"\n",
    "                               WITH first as (\n",
    "                                    SELECT user_id, MIN(event_time) AS first_interaction_time\n",
    "                                    FROM ecommerce\n",
    "                                    GROUP BY user_id\n",
    "                                )\n",
    "                                SELECT DATE(DATE_TRUNC('week',first_interaction_time)) AS WEEK, \n",
    "                                CAST(DATEDIFF(s.event_time, f.first_interaction_time) / 7 AS INT) AS diff_week,\n",
    "                                COUNT(DISTINCT s.user_id) as user_cnt\n",
    "                                FROM ecommerce s LEFT JOIN first f\n",
    "                                ON s.user_id = f.user_id\n",
    "                                GROUP BY WEEK, diff_week\n",
    "                                ORDER BY diff_week\n",
    "\n",
    "                          \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retention_range_7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas, pivot 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retention_7 = retention_range_7.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retention_7.sort_values(by=['WEEK','diff_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retention_7_pivot = df_retention_7.pivot(index='WEEK',columns='diff_week',values='user_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retention_7_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### value 비율로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_retention_7_pivot)):\n",
    "    a = df_retention_7_pivot.iloc[i,0]\n",
    "    for j in range(len(df_retention_7_pivot.columns)):\n",
    "        if df_retention_7_pivot.isnull().iloc[i,j]:\n",
    "            pass\n",
    "        else:\n",
    "            df_retention_7_pivot.iloc[i,j] = round(df_retention_7_pivot.iloc[i,j] / a, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retention_7_pivot.to_csv(\"주간간리텐션.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주간 리텐션 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(df_retention_7_pivot, annot=True, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
